{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Product Pricer Continued**  \n",
    "\n",
    "A model that estimates the cost of a product based on its description.  \n",
    "\n",
    "### **Data Curation - Part 2**  \n",
    "\n",
    "In this session, we will expand our dataset for better coverage and refine it into a high-quality dataset for training.  \n",
    "\n",
    "**Why is data curation important?**  \n",
    "Although it may not seem as exciting as model training, data curation is a **crucial skill** for LLM engineers. A well-crafted dataset ensures better model performance and can help you develop commercial AI solutions with real-world applications.  \n",
    "\n",
    "#### **Dataset Location**  \n",
    "The dataset is available here:  \n",
    "🔗 [Amazon Reviews 2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023)  \n",
    "\n",
    "For access to all product categories:  \n",
    "🔗 [Meta Categories Folder](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories)  \n",
    "\n",
    "\n",
    "### **⚠️ Important Note**  \n",
    "\n",
    "We are working with a **large dataset** of **400,000 items**, covering multiple product categories.  \n",
    "- Training this dataset in **Week 7** could take **20+ hours** (depending on GPU power) and might incur **cloud computing costs**.  \n",
    "- For a **quicker and cost-effective** alternative, use the **\"lite\"** dataset focused on *Home Appliances*. This ensures you cover all learning objectives with a **smaller dataset**.  \n",
    "\n",
    "Alternatively, you can **skip the curation process** by downloading the pre-processed dataset here:  \n",
    "🔗 [Pickle Files](https://drive.google.com/drive/folders/1f_IZGybvs9o0J5sb3xmtTEQB3BXllzrW)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Environment Setup**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Check if API keys are properly loaded\n",
    "if not openai_api_key or not hf_token:\n",
    "    raise ValueError(\"Missing API keys. Ensure OPENAI_API_KEY and HF_TOKEN are set in the .env file.\")\n",
    "\n",
    "# Set environment variables explicitly (optional)\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(hf_token, add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils.items import Item\n",
    "from utils.loaders import ItemLoader\n",
    "\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Load Product Data**  \n",
    "\n",
    "We start by loading product data from the **\"Appliances\"** category to check if the dataset is working correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the same dataset as last time\n",
    "\n",
    "items = ItemLoader(\"Appliances\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for a familiar item..\n",
    "print(items[1].prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scaling Up to Multiple Categories**  \n",
    "\n",
    "Now, let's expand the dataset to include **various product categories** (excluding clothing, beauty, and books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"Automotive\",\n",
    "    \"Electronics\",\n",
    "    \"Office_Products\",\n",
    "    \"Tools_and_Home_Improvement\",\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Toys_and_Games\",\n",
    "    \"Appliances\",\n",
    "    \"Musical_Instruments\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items = []\n",
    "for dataset_name in dataset_names:\n",
    "    loader = ItemLoader(dataset_name)\n",
    "    items.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A grand total of {len(items):,} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Data Exploration**  \n",
    "\n",
    "#### **Token Count Distribution** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of token counts\n",
    "\n",
    "tokens = [item.token_count for item in items]\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(tokens, rwidth=0.7, color=\"skyblue\", bins=range(0, 300, 10))\n",
    "plt.xlabel('Length (tokens)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f\"Token counts: Avg {sum(tokens)/len(tokens):,.1f} and highest {max(tokens):,}\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Price Distribution**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of prices\n",
    "\n",
    "prices = [item.price for item in items]\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(prices, rwidth=0.7, color=\"blueviolet\", bins=range(0, 1000, 10))\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f\"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Category Distribution**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = Counter()\n",
    "for item in items:\n",
    "    category_counts[item.category]+=1\n",
    "\n",
    "categories = category_counts.keys()\n",
    "counts = [category_counts[category] for category in categories]\n",
    "\n",
    "# Bar chart by category\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(categories, counts, color=\"goldenrod\")\n",
    "plt.title('How many in each category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v, f\"{v:,}\", ha='center', va='bottom')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Dataset Balancing & Curation**  \n",
    "\n",
    "We will **rebalance** the dataset to:  \n",
    "- Reduce **overrepresentation** of low-cost items.  \n",
    "- Increase **average price** to be above **$60**.  \n",
    "- **Limit** the dominance of \"Automotive\" category.  \n",
    "\n",
    "#### **Hint: Let's create a dictionary with a key of each price from $1 to $999 And in the value, put a list of items with that price (to nearest round number)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = defaultdict(list)\n",
    "for item in items:\n",
    "    slots[round(item.price)].append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create a dataset called \"sample\" which tries to more evenly take from the range of prices and gives more weight to items from categories other than `Automotive`, and then set random seed for reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Step 3: Initialize an empty list to store the final sampled dataset\n",
    "sample = []\n",
    "\n",
    "# Step 4: Iterate through price slots from 1 to 999\n",
    "for i in range(1, 1000):\n",
    "    slot = slots[i]  # Retrieve the list of items in the current price slot\n",
    "    \n",
    "    # Step 5: If price is >= 240, include all items (no filtering)\n",
    "    if i >= 240:\n",
    "        sample.extend(slot)  \n",
    "\n",
    "    # Step 6: If the slot contains 1200 items or fewer, include all items\n",
    "    elif len(slot) <= 1200:\n",
    "        sample.extend(slot)  \n",
    "\n",
    "    # Step 7: If the slot has more than 1200 items, perform weighted sampling\n",
    "    else:\n",
    "        # Assign a sampling weight: \n",
    "        # 'Automotive' items get weight 1, all others get weight 5 (favoring non-automotive)\n",
    "        weights = np.array([1 if item.category == 'Automotive' else 5 for item in slot])\n",
    "\n",
    "        # Normalize the weights to sum to 1 for proper probability distribution\n",
    "        weights = weights / np.sum(weights)  \n",
    "\n",
    "        # Randomly select 1200 items based on the assigned weights\n",
    "        selected_indices = np.random.choice(len(slot), size=1200, replace=False, p=weights)\n",
    "\n",
    "        # Add the selected items to the final sample\n",
    "        sample.extend([slot[i] for i in selected_indices])\n",
    "\n",
    "print(f\"Final dataset contains {len(sample):,} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Final Dataset Check**  \n",
    "\n",
    "#### **Price Distribution (Final Version)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of prices in sample\n",
    "\n",
    "prices = [item.price for item in sample]\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(prices, rwidth=0.7, color=\"darkblue\", bins=range(0, 1000, 10))\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f\"Avg Price: {sum(prices)/len(prices):.2f}, Highest Price: {max(prices):,.2f}\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We did well in terms of raising the average price and having a 'smooth-ish' population of prices. Let's now see the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = Counter()\n",
    "for item in sample:\n",
    "    category_counts[item.category]+=1\n",
    "\n",
    "categories = category_counts.keys()\n",
    "counts = [category_counts[category] for category in categories]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(categories, counts, color=\"lightgreen\")\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('How many in each category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v, f\"{v:,}\", ha='center', va='bottom')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(item):\n",
    "    prompt = item.prompt\n",
    "    tokens = Item.tokenizer.encode(item.prompt)\n",
    "    print(prompt)\n",
    "    print(tokens[-10:])\n",
    "    print(Item.tokenizer.batch_decode(tokens[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(sample[398000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(sample[40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Train-Test Split & Upload**  \n",
    "\n",
    "#### **Create Train-Test Split** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(sample)\n",
    "train, test = sample[:400_000], sample[400_000:402_000]\n",
    "print(f\"Training set: {len(train):,} items | Test set: {len(test):,} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0].test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of prices in the first 250 test points\n",
    "\n",
    "prices = [float(item.price) for item in test[:250]]\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.title(f\"Avg {sum(prices)/len(prices):.2f} and highest {max(prices):,.2f}\\n\")\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(prices, rwidth=0.7, color=\"darkblue\", bins=range(0, 1000, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Data as Hugging Face Dataset**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = [item.prompt for item in train]\n",
    "train_prices = [item.price for item in train]\n",
    "test_prompts = [item.test_prompt() for item in test]\n",
    "test_prices = [item.price for item in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from the lists\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_prompts, \"price\": train_prices})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_prompts, \"price\": test_prices})\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USER = \"your-hf-user-name\"\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "dataset.push_to_hub(DATASET_NAME, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(\"your_hf_username/pricer-data\", private=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save Locally as Pickle Files**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'wb') as file:\n",
    "    pickle.dump(train, file)\n",
    "\n",
    "with open('test.pkl', 'wb') as file:\n",
    "    pickle.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
